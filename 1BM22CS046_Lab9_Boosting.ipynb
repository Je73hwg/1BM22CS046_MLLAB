{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1kCdkWLCqqwR1wskwqsiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Je73hwg/1BM22CS046_MLLAB/blob/main/1BM22CS046_Lab9_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsoZB-YawBjm",
        "outputId": "39027bfe-a6f2-4d43-9af2-7d66a4c459c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "Confusion Matrix:\n",
            "[[7003  411]\n",
            " [1223 1132]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_file.csv' with the path to your CSV file\n",
        "data = pd.read_csv('/content/income.csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost classifier with a Decision Tree as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # You can adjust the max_depth\n",
        "model = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_file.xlsx' with the path to your Excel file\n",
        "data = pd.read_csv('/content/iris (1).csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to evaluate AdaBoost with different estimators\n",
        "def evaluate_adaboost(estimator, n_estimators_list, learning_rates):\n",
        "    for n_estimators in n_estimators_list:\n",
        "        for learning_rate in learning_rates:\n",
        "            # Create the AdaBoost model\n",
        "            model = AdaBoostClassifier(estimator=estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate accuracy score\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f'Estimator: {estimator.__class__.__name__}, n_estimators: {n_estimators}, learning_rate: {learning_rate}, Accuracy: {accuracy:.2f}')\n",
        "\n",
        "            # Generate confusion matrix\n",
        "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "            print('Confusion Matrix:')\n",
        "            print(conf_matrix)\n",
        "            print('-' * 50)\n",
        "\n",
        "# Define the range of n_estimators and learning rates to test\n",
        "n_estimators_list = [50, 100, 150]\n",
        "learning_rates = [0.01, 0.1, 1.0]\n",
        "\n",
        "# Evaluate AdaBoost with Decision Tree as the base estimator\n",
        "print(\"Evaluating AdaBoost with Decision Tree:\")\n",
        "evaluate_adaboost(DecisionTreeClassifier(max_depth=1), n_estimators_list, learning_rates)\n",
        "\n",
        "# Evaluate AdaBoost with Linear Regression as the base estimator (using AdaBoostRegressor)\n",
        "print(\"\\nEvaluating AdaBoost with Linear Regression:\")\n",
        "evaluate_adaboost(LinearRegression(), n_estimators_list, learning_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HHmQX6tcyOWo",
        "outputId": "a85044b8-18a3-482b-e40c-e16aac5f9390"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating AdaBoost with Decision Tree:\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.01, Accuracy: 0.63\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0 11  0]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating AdaBoost with Linear Regression:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'setosa'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1b11941042b6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Evaluate AdaBoost with Linear Regression as the base estimator (using AdaBoostRegressor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating AdaBoost with Linear Regression:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mevaluate_adaboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-1b11941042b6>\u001b[0m in \u001b[0;36mevaluate_adaboost\u001b[0;34m(estimator, n_estimators_list, learning_rates)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'setosa'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_file.xlsx' with the path to your Excel file\n",
        "data = pd.read_csv('/content/iris (1).csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Encode categorical target variable if it's a classification problem\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to evaluate AdaBoost with different estimators\n",
        "def evaluate_adaboost(estimator, n_estimators_list, learning_rates):\n",
        "    for n_estimators in n_estimators_list:\n",
        "        for learning_rate in learning_rates:\n",
        "            # Create the AdaBoost model\n",
        "            model = AdaBoostClassifier(estimator=estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate accuracy score\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f'Estimator: {estimator.__class__.__name__}, n_estimators: {n_estimators}, learning_rate: {learning_rate}, Accuracy: {accuracy:.2f}')\n",
        "\n",
        "            # Generate confusion matrix\n",
        "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "            print('Confusion Matrix:')\n",
        "            print(conf_matrix)\n",
        "            print('-' * 50)\n",
        "\n",
        "# Define the range of n_estimators and learning rates to test\n",
        "n_estimators_list = [50, 100, 150]\n",
        "learning_rates = [0.01, 0.1, 1.0]\n",
        "\n",
        "# Evaluate AdaBoost with Decision Tree as the base estimator\n",
        "print(\"Evaluating AdaBoost with Decision Tree:\")\n",
        "evaluate_adaboost(DecisionTreeClassifier(max_depth=1), n_estimators_list, learning_rates)\n",
        "\n",
        "# Evaluate AdaBoost with Linear Regression as the base estimator (using AdaBoostRegressor)\n",
        "print(\"\\nEvaluating AdaBoost with Linear Regression:\")\n",
        "evaluate_adaboost(LinearRegression(), n_estimators_list, learning_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uYJC_uvGzHe_",
        "outputId": "926a4d55-8e2f-4993-ca17-7df06f9f08e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating AdaBoost with Decision Tree:\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.01, Accuracy: 0.63\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0 11  0]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating AdaBoost with Linear Regression:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ceec2b1494c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Evaluate AdaBoost with Linear Regression as the base estimator (using AdaBoostRegressor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating AdaBoost with Linear Regression:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mevaluate_adaboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ceec2b1494c5>\u001b[0m in \u001b[0;36mevaluate_adaboost\u001b[0;34m(estimator, n_estimators_list, learning_rates)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# Instances incorrectly classified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_file.csv' with the path to your CSV file\n",
        "data = pd.read_csv('/content/iris (1).csv')\n",
        "\n",
        "# Check if the data is loaded correctly\n",
        "if data is None or data.empty:\n",
        "    raise ValueError(\"The dataset is empty or not loaded correctly.\")\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Encode categorical target variable if it's a classification problem\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to evaluate AdaBoost with different estimators\n",
        "def evaluate_adaboost(estimator, n_estimators_list, learning_rates):\n",
        "    for n_estimators in n_estimators_list:\n",
        "        for learning_rate in learning_rates:\n",
        "            # Create the AdaBoost model\n",
        "            model = AdaBoostClassifier(estimator=estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate accuracy score\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f'Estimator: {estimator.__class__.__name__}, n_estimators: {n_estimators}, learning_rate: {learning_rate}, Accuracy: {accuracy:.2f}')\n",
        "\n",
        "            # Generate confusion matrix\n",
        "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "            print('Confusion Matrix:')\n",
        "            print(conf_matrix)\n",
        "            print('-' * 50)\n",
        "\n",
        "# Define the range of n_estimators and learning rates to test\n",
        "n_estimators_list = [50, 100, 150]\n",
        "learning_rates = [0.01, 0.1, 1.0]\n",
        "\n",
        "# Evaluate AdaBoost with Decision Tree as the base estimator\n",
        "print(\"Evaluating AdaBoost with Decision Tree:\")\n",
        "evaluate_adaboost(DecisionTreeClassifier(max_depth=1), n_estimators_list, learning_rates)\n",
        "\n",
        "# Evaluate AdaBoost with Logistic Regression as the base estimator\n",
        "print(\"\\nEvaluating AdaBoost with Logistic Regression:\")\n",
        "evaluate_adaboost(LogisticRegression(max_iter=200), n_estimators_list, learning_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp2DzKC_zRk3",
        "outputId": "f59a5c51-1fd9-4d40-d848-e952a4d63b51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating AdaBoost with Decision Tree:\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.01, Accuracy: 0.63\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0 11  0]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 50, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 100, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.01, Accuracy: 0.97\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: DecisionTreeClassifier, n_estimators: 150, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  1 10]]\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating AdaBoost with Logistic Regression:\n",
            "Estimator: LogisticRegression, n_estimators: 50, learning_rate: 0.01, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 50, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 50, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  7  2]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 100, learning_rate: 0.01, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 100, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 100, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  7  2]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 150, learning_rate: 0.01, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 150, learning_rate: 0.1, Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n",
            "Estimator: LogisticRegression, n_estimators: 150, learning_rate: 1.0, Accuracy: 0.93\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  7  2]\n",
            " [ 0  0 11]]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BaiaKqKzjvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}